{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Problem Set 4\n",
    "Designed by Ben Usman, Kun He, and Sarah Adel Bargal, with help from Kate Saenko and Brian Kulis.\n",
    "\n",
    "This assignment will introduce you to:\n",
    "1. Building and training a convolutional network\n",
    "2. Saving snapshots of your trained model\n",
    "3. Reloading weights from a saved model\n",
    "4. Fine-tuning a pre-trained network\n",
    "5. Visualizations using Tensorboard\n",
    "\n",
    "This code has been tested and should for Python 3.5 and 2.7 with tensorflow 1.0.*. Since recently, you can update to recent tensorflow version just by doing `pip install tensorflow`,  or `pip install tensorflow-gpu` if you want to use GPU.\n",
    "\n",
    "**Note:** This notebook contains problem descriptions and demo/starter code. However, you're welcome to implement and submit .py files directly, if that's easier for you. Starter .py files are provided in the same `pset4/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 0: Tutorials\n",
    "\n",
    "You will find these TensorFlow tutorials on CNNs useful:\n",
    " - [Deep MNIST for experts](https://www.tensorflow.org/get_started/mnist/pros)\n",
    " - [Convolutional Neural Networks](https://www.tensorflow.org/tutorials/deep_cnn)\n",
    " \n",
    "Note that there are many ways to implement the same thing in TensorFlow, for example, both tf.nn and tf.layers provide convolutional layers but with slightly different interfaces. You will need to read the documentation of the functions provided below to understand how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1: Building and Training a ConvNet on SVHN\n",
    "(25 points)\n",
    "\n",
    "First we provide demo code that trains a convolutional network on the [SVHN Dataset](http://ufldl.stanford.edu/housenumbers/).. \n",
    "\n",
    "You will need to download   __Format 2__ from the link above.\n",
    "- Create a directory named `svhn_mat/` in the working directory. Or, you can create it anywhere you want, but change the path in `svhn_dataset_generator` to match it.\n",
    "- Download `train_32x32.mat` and `test_32x32.mat` to this directory.\n",
    "- `extra_32x32.mat` is NOT needed.\n",
    "- You may find the `wget` command useful for downloading on linux. \n",
    "\n",
    "\n",
    "\n",
    "The following defines a generator for the SVHN Dataset, yielding the next batch every time next is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "\n",
    "from six.moves import range\n",
    "\n",
    "import read_data\n",
    "\n",
    "@read_data.restartable\n",
    "def svhn_dataset_generator(dataset_name, batch_size):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    path = './svhn_mat/' # path to the SVHN dataset you will download in Q1.1\n",
    "    file_name = '%s_32x32.mat' % dataset_name\n",
    "    file_dict = scipy.io.loadmat(os.path.join(path, file_name))\n",
    "    X_all = file_dict['X'].transpose((3, 0, 1, 2))\n",
    "    y_all = file_dict['y']\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    y_all_padded[y_all_padded == 10] = 0\n",
    "    \n",
    "    for slice_i in range(int(math.ceil(data_len / batch_size))):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines the CovNet Model. It has two identical conv layers with 32 5x5 convlution filters, followed by a fully-connected layer to output the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def cnn_map(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat') # What is pool2flat ?\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def apply_classification_loss(model_function):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/cpu:0\"):  # use gpu:0 if on GPU\n",
    "            x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            y_ = tf.placeholder(tf.int32, [None])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            y_dict = dict(labels=y_, logits=y_logits)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            trainer = tf.train.AdamOptimizer()\n",
    "            train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_) # Cast a tensor to a new type.\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            # correct_prediction is a boolen, it's cast to a float.\n",
    "            # reduce_mean is a misnomer, it just calculates the mean.\n",
    "    \n",
    "    model_dict = {'graph': g, 'inputs': [x_, y_], 'train_op': train_op,\n",
    "                  'accuracy': accuracy, 'loss': cross_entropy_loss}\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q1.2 Training SVHN Net\n",
    "Now we train a `cnn_map` net on Format 2 of the SVHN Dataset. We will call this \"SVHN net\". \n",
    "\n",
    "**Note:** training will take a while, so you might want to use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_model(model_dict, dataset_generators, epoch_n, print_every):\n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(model_dict['train_op'], feed_dict=train_feed_dict)\n",
    "                \n",
    "                if iter_i % print_every == 0:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict))\n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i, iter_i, ) + tuple(averages)\n",
    "                    print('epoch {:d} iter {:d}, loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_generators = {\n",
    "        'train': svhn_dataset_generator('train', 256),\n",
    "        'test': svhn_dataset_generator('test', 256)\n",
    "}\n",
    "    \n",
    "model_dict = apply_classification_loss(cnn_map)\n",
    "train_model(model_dict, dataset_generators, epoch_n=50, print_every=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q1.3 SVHN Net Variations\n",
    "Now we vary the structure of the network. To keep things simple, we still use  two identical conv layers, but vary their parameters. \n",
    "\n",
    "Report the final test accuracy on 3 different number of filters, and 3 different number of strides. Each time when you vary one parameter, keep the other fixed at the original value.\n",
    "\n",
    "|Stride|Accuracy|\n",
    "|--|-------------------------------|\n",
    "| / | / |\n",
    "| / | / |\n",
    "| / | / |\n",
    "\n",
    "|Filters|Accuracy|\n",
    "|--|-------------------------------|\n",
    "| / | / |\n",
    "| / | / |\n",
    "| / | / |\n",
    "\n",
    "A template for one sample modification is given below. \n",
    "\n",
    "**Note:** you're welcome to decide how many training epochs to use, if that gets you the same results but faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cnn_modification(x_):\n",
    "    raise NotImplemented(\"Add your code here!\")\n",
    "        \n",
    "    ###################################\n",
    "    ####     ADD YOUR CODE HERE    ####\n",
    "    ###################################\n",
    "\n",
    "modified_model_dict = apply_classification_loss(cnn_modification)\n",
    "train_model(modified_model_dict, dataset_generators, epoch_n=100, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: Saving and Reloading Model Weights\n",
    "(25 points)\n",
    "\n",
    "In this section you learn to save the weights of a trained model, and to load the weights of a saved model. This is really useful when we would like to load an already trained model in order to continue training or to fine-tune it. Often times we save “snapshots” of the trained model as training progresses in case the training is interrupted, or in case we would like to fall back to an earlier model, this is called snapshot saving.\n",
    "\n",
    "### Q2.1 Defining another network\n",
    "Define a network with a slightly different structure in `def cnn_expanded(x_)` below. `cnn_expanded` is an expanded version of `cnn_model`. \n",
    "It should have: \n",
    "- a different size of kernel for the last convolutional layer, \n",
    "- followed by one additional convolutional layer, and \n",
    "- followed by one additional pooling layer.\n",
    "\n",
    "The last fully-connected layer will stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the new model (see cnn_map(x_) above for an example)\n",
    "def cnn_expanded(x_):\n",
    "    \n",
    "     conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[7, 7],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=64, # number of filters\n",
    "            kernel_size=[7, 7],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)  # convolution stride\n",
    "    \n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool3, scope='pool3flat') # What is pool2flat ?\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q2.2 Saving and Loading Weights\n",
    "`new_train_model()` below has two additional parameters `save_model=False, load_model=False` than `train_model` defined previously. Modify `new_train_model()` such that it would \n",
    "- save weights after the training is complete if `save_model` is `True`, and\n",
    "- load weights on start-up before training if `load_model` is `True`.\n",
    "\n",
    "*Hint:*  take a look at the docs for `tf.train.Saver()` here: https://www.tensorflow.org/api_docs/python/tf/train/Saver#__init__. You probably will be specifying the first argument `var_list` to accomplish this question.\n",
    "\n",
    "Note: if you are unable to load weights into `cnn_expanded` network, use `cnn_map` in order to continue the assingment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### Modify this:\n",
    "def new_train_model(model_dict, dataset_generators, epoch_n, print_every,\n",
    "                    save_model=False, load_model=False):\n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(model_dict['train_op'], feed_dict=train_feed_dict)\n",
    "                \n",
    "                if iter_i % print_every == 0:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict))\n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i, iter_i, ) + tuple(averages)\n",
    "                    print('iteration {:d} {:d}\\t loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))\n",
    "    \n",
    "\n",
    "def test_saving():\n",
    "    ### Hint: call the saver like this: tf.train.Saver(var_list)\n",
    "    ### where var_list is a list of TF variables you want to save (from conv1 layer)\n",
    "    model_dict = apply_classification_loss(cnn_map)\n",
    "    new_train_model(model_dict, dataset_generators, epoch_n=100, print_every=10, save_model=True)\n",
    "    ### Hint: call the saver like this: tf.train.Saver(var_list)\n",
    "    ### where var_list is a list of TF variables you want to load from the checkpoint (for conv1 layer)\n",
    "    cnn_expanded_dict = apply_classification_loss(cnn_expanded)\n",
    "    new_train_model(cnn_expanded_dict, dataset_generators, epoch_n=10, print_every=1, load_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3: Fine-tuning a Pre-trained Network on CIFAR-10\n",
    "(20 points)\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is another popular benchmark for image classification.\n",
    "We provide you with modified verstion of the file cifar10.py from [https://github.com/Hvass-Labs/TensorFlow-Tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import read_cifar10 as cf10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We also provide a generator for the CIFAR-10 Dataset, yielding the next batch every time next is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@read_data.restartable\n",
    "def cifar10_dataset_generator(dataset_name, batch_size, restrict_size=1000):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    X_all_unrestricted, y_all = (cf10.load_training_data() if dataset_name == 'train'\n",
    "                                 else cf10.load_test_data())\n",
    "    \n",
    "    actual_restrict_size = restrict_size if dataset_name == 'train' else int(1e10)\n",
    "    X_all = X_all_unrestricted[:actual_restrict_size]\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    \n",
    "    for slice_i in range(math.ceil(data_len / batch_size)):\n",
    "        idx = slice_i * batch_size\n",
    "        #X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]*255  # bugfix: thanks Zezhou Sun!\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch.astype(np.uint8), y_batch.astype(np.uint8)\n",
    "\n",
    "cifar10_dataset_generators = {\n",
    "    'train': cifar10_dataset_generator('train', 1000),\n",
    "    'test': cifar10_dataset_generator('test', -1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q3.1 Fine-tuning\n",
    "Let's fine-tune SVHN net on **1000 examples** from CIFAR-10. \n",
    "Compare test accuracies of the following scenarios: \n",
    "  - Training `cnn_map` from scratch on the 1000 CIFAR-10 examples\n",
    "  - Fine-tuning SVHN net (`cnn_map` trained on SVHN dataset) on 1000 exampes from CIFAR-10. Use `new_train_model()` defined above to load SVHN net weights, but train on the CIFAR-10 examples.\n",
    "  \n",
    "**Important:** please do not change the `restrict_size=1000` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## train a model from scratch\n",
    "new_train_model(cnn_expanded_dict, cifar10_dataset_generators, epoch_n=100, \n",
    "                print_every=10, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## fine-tuning SVHN Net using Cifar-10 weights saved in Q2\n",
    "new_train_model(cnn_expanded_dict, cifar10_dataset_generators, epoch_n=100, \n",
    "                print_every=10, load_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 4: TensorBoard\n",
    "(30 points)\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is a very helpful tool for visualization of neural networks. \n",
    "\n",
    "### Q4.1 Plotting\n",
    "Present at least one visualization for each of the following:\n",
    "  - Filters\n",
    "  - Loss\n",
    "  - Accuracy\n",
    "\n",
    "Modify code you have wrote above to also have summary writers. To  run tensorboard, the command is `tensorboard --logdir=path/to/your/log/directory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Filter, loss, and accuracy visualizations\n",
    "def visualize():\n",
    "    raise NotImplemented(\"Add your code here!\")\n",
    "        \n",
    "    ###################################\n",
    "    ####     ADD YOUR CODE HERE    ####\n",
    "    ###################################    \n",
    "    \n",
    "visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 5: Bonus\n",
    "(20 points)\n",
    "\n",
    "### Q5.1 SVHN Net ++\n",
    "Improve the accuracy of SVHN Net beyond that of the provided demo: SVHN Net ++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def SVHN_plusplus():\n",
    "    raise NotImplemented(\"Add your code here!\")\n",
    "        \n",
    "    ###################################\n",
    "    ####     ADD YOUR CODE HERE    ####\n",
    "    ###################################    \n",
    "    \n",
    "SVHN_plusplus()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
